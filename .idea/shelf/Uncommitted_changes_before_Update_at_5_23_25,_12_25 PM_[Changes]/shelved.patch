Index: model.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import math\nimport struct\nimport inspect\nfrom dataclasses import dataclass\nfrom typing import Any, Optional, Tuple\ntry:\n    import bitsandbytes as bnb\nexcept ImportError:\n    bnb = None\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\n@dataclass\nclass ModelArgs:\n    # default hyperparameters for the Llama 7B model\n    dim: int = 4096\n    n_layers: int = 32\n    n_heads: int = 32\n    n_kv_heads: Optional[int] = None\n    vocab_size: int = 32000\n    hidden_dim: Optional[int] = None\n    multiple_of: int = 256  # MLP hidden layer size will be multiple of\n    norm_eps: float = 1e-5\n    max_seq_len: int = 2048\n    dropout: float = 0.0\n\n\nclass RMSNorm(torch.nn.Module):\n    def __init__(self, dim: int, eps: float):\n        super().__init__()\n        self.eps = eps\n        self.weight = nn.Parameter(torch.ones(dim))\n\n    def _norm(self, x):\n        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n\n    def forward(self, x):\n        output = self._norm(x.float()).type_as(x)\n        return output * self.weight\n\n\ndef precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n    t = torch.arange(end, device=freqs.device)  # type: ignore\n    freqs = torch.outer(t, freqs).float()  # type: ignore\n    freqs_cos = torch.cos(freqs)  # real part\n    freqs_sin = torch.sin(freqs)  # imaginary part\n    return freqs_cos, freqs_sin\n\ndef reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n    ndim = x.ndim\n    assert 0 <= 1 < ndim\n    assert freqs_cis.shape == (x.shape[1], x.shape[-1])\n    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n    return freqs_cis.view(shape)\n\ndef apply_rotary_emb(\n    xq: torch.Tensor,\n    xk: torch.Tensor,\n    freqs_cos: torch.Tensor,\n    freqs_sin: torch.Tensor\n) -> Tuple[torch.Tensor, torch.Tensor]:\n\n    # reshape xq and xk to match the complex representation\n    xq_r, xq_i = xq.float().reshape(xq.shape[:-1] + (-1, 2)).unbind(-1)\n    xk_r, xk_i = xk.float().reshape(xk.shape[:-1] + (-1, 2)).unbind(-1)\n\n    # reshape freqs_cos and freqs_sin for broadcasting\n    freqs_cos = reshape_for_broadcast(freqs_cos, xq_r)\n    freqs_sin = reshape_for_broadcast(freqs_sin, xq_r)\n\n    # apply rotation using real numbers\n    xq_out_r = xq_r * freqs_cos - xq_i * freqs_sin\n    xq_out_i = xq_r * freqs_sin + xq_i * freqs_cos\n    xk_out_r = xk_r * freqs_cos - xk_i * freqs_sin\n    xk_out_i = xk_r * freqs_sin + xk_i * freqs_cos\n\n    # flatten last two dimensions\n    xq_out = torch.stack([xq_out_r, xq_out_i], dim=-1).flatten(3)\n    xk_out = torch.stack([xk_out_r, xk_out_i], dim=-1).flatten(3)\n\n    return xq_out.type_as(xq), xk_out.type_as(xk)\n\ndef repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n    \"\"\"torch.repeat_interleave(x, dim=2, repeats=n_rep)\"\"\"\n    bs, slen, n_kv_heads, head_dim = x.shape\n    if n_rep == 1:\n        return x\n    return (\n        x[:, :, :, None, :]\n        .expand(bs, slen, n_kv_heads, n_rep, head_dim)\n        .reshape(bs, slen, n_kv_heads * n_rep, head_dim)\n    )\n\nclass Attention(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n        self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n        assert args.n_heads % self.n_kv_heads == 0\n        model_parallel_size = 1\n        self.n_local_heads = args.n_heads // model_parallel_size\n        self.n_local_kv_heads = self.n_kv_heads // model_parallel_size\n        self.n_rep = self.n_local_heads // self.n_local_kv_heads\n        self.head_dim = args.dim // args.n_heads\n        self.wq = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False)\n        self.wk = nn.Linear(args.dim, self.n_kv_heads * self.head_dim, bias=False)\n        self.wv = nn.Linear(args.dim, self.n_kv_heads * self.head_dim, bias=False)\n        self.wo = nn.Linear(args.n_heads * self.head_dim, args.dim, bias=False)\n        self.attn_dropout = nn.Dropout(args.dropout)\n        self.resid_dropout = nn.Dropout(args.dropout)\n        self.dropout = args.dropout\n\n        # use flash attention or a manual implementation?\n        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n        if not self.flash:\n            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n            mask = torch.full((1, 1, args.max_seq_len, args.max_seq_len), float(\"-inf\"))\n            mask = torch.triu(mask, diagonal=1)\n            self.register_buffer(\"mask\", mask)\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        freqs_cos: torch.Tensor,\n        freqs_sin: torch.Tensor,\n    ):\n        bsz, seqlen, _ = x.shape\n\n        # QKV\n        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)\n        xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n        xk = xk.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)\n        xv = xv.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)\n\n        # RoPE relative positional embeddings\n        xq, xk = apply_rotary_emb(xq, xk, freqs_cos, freqs_sin)\n\n        # grouped multiquery attention: expand out keys and values\n        xk = repeat_kv(xk, self.n_rep)  # (bs, seqlen, n_local_heads, head_dim)\n        xv = repeat_kv(xv, self.n_rep)  # (bs, seqlen, n_local_heads, head_dim)\n\n        # make heads into a batch dimension\n        xq = xq.transpose(1, 2)  # (bs, n_local_heads, seqlen, head_dim)\n        xk = xk.transpose(1, 2)\n        xv = xv.transpose(1, 2)\n\n        # flash implementation\n        if self.flash:\n            output = torch.nn.functional.scaled_dot_product_attention(xq, xk, xv, attn_mask=None, dropout_p=self.dropout if self.training else 0.0, is_causal=True,enable_gqa=True)\n        else:\n            # manual implementation\n            scores = torch.matmul(xq, xk.transpose(2, 3)) / math.sqrt(self.head_dim)\n            assert hasattr(self, 'mask')\n            scores = scores + self.mask[:, :, :seqlen, :seqlen]   # (bs, n_local_heads, seqlen, cache_len + seqlen)\n            scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n            scores = self.attn_dropout(scores)\n            output = torch.matmul(scores, xv)  # (bs, n_local_heads, seqlen, head_dim)\n\n        # restore time as batch dimension and concat heads\n        output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)\n\n        # final projection into the residual stream\n        output = self.wo(output)\n        output = self.resid_dropout(output)\n        return output\n\n\nclass FeedForward(nn.Module):\n    def __init__(self, dim: int, hidden_dim: int, multiple_of: int, dropout: float):\n        super().__init__()\n        if hidden_dim is None:\n            hidden_dim = 4 * dim\n            hidden_dim = int(2 * hidden_dim / 3)\n            hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n        self.w1 = nn.Linear(dim, hidden_dim, bias=False)\n        self.w2 = nn.Linear(hidden_dim, dim, bias=False)\n        self.w3 = nn.Linear(dim, hidden_dim, bias=False)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        return self.dropout(self.w2(F.silu(self.w1(x)) * self.w3(x)))\n\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, layer_id: int, args: ModelArgs):\n        super().__init__()\n        self.n_heads = args.n_heads\n        self.dim = args.dim\n        self.head_dim = args.dim // args.n_heads\n        self.attention = Attention(args)\n        self.feed_forward = FeedForward(\n            dim=args.dim,\n            hidden_dim=args.hidden_dim,\n            multiple_of=args.multiple_of,\n            dropout=args.dropout,\n        )\n        self.layer_id = layer_id\n        self.attention_norm = RMSNorm(args.dim, eps=args.norm_eps)\n        self.ffn_norm = RMSNorm(args.dim, eps=args.norm_eps)\n\n    def forward(self, x, freqs_cos, freqs_sin):\n        h = x + self.attention.forward(self.attention_norm(x), freqs_cos, freqs_sin)\n        out = h + self.feed_forward.forward(self.ffn_norm(h))\n        return out\n\n\nclass Transformer(nn.Module):\n    last_loss: Optional[torch.Tensor]\n\n    def __init__(self, params: ModelArgs):\n        super().__init__()\n        self.params = params\n        self.vocab_size = params.vocab_size\n        self.n_layers = params.n_layers\n\n        self.tok_embeddings = nn.Embedding(params.vocab_size, params.dim)\n        self.dropout = nn.Dropout(params.dropout)\n        self.layers = torch.nn.ModuleList()\n        for layer_id in range(params.n_layers):\n            self.layers.append(TransformerBlock(layer_id, params))\n        self.norm = RMSNorm(params.dim, eps=params.norm_eps)\n        self.output = nn.Linear(params.dim, params.vocab_size, bias=False)\n\n        # share the unembedding parameters with the embedding parameters\n        self.tok_embeddings.weight = self.output.weight # https://paperswithcode.com/method/weight-tying\n\n        # some useful precompute for the RoPE relative positional embeddings\n        freqs_cos, freqs_sin = precompute_freqs_cis(self.params.dim // self.params.n_heads, self.params.max_seq_len)\n        self.register_buffer(\"freqs_cos\", freqs_cos, persistent=False)\n        self.register_buffer(\"freqs_sin\", freqs_sin, persistent=False)\n\n        # init all weights\n        self.apply(self._init_weights)\n        # apply special scaled init to the residual projections, per GPT-2 paper\n        for pn, p in self.named_parameters():\n            if pn.endswith('w3.weight') or pn.endswith('wo.weight'):\n                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * params.n_layers))\n\n        # Initialize attribute for the loss of the last forward call. This will be set if the forward is called with a targets tensor.\n        self.last_loss = None\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\n    def forward(self, tokens: torch.Tensor, targets: Optional[torch.Tensor] = None) -> torch.Tensor:\n        _bsz, seqlen = tokens.shape\n        h = self.tok_embeddings(tokens)\n        h = self.dropout(h)\n        freqs_cos = self.freqs_cos[:seqlen]\n        freqs_sin = self.freqs_sin[:seqlen]\n\n        for layer in self.layers:\n            h = layer(h, freqs_cos, freqs_sin)\n        h = self.norm(h)\n\n        if targets is not None:\n            # if we are given some desired targets also calculate the loss\n            logits = self.output(h)\n            self.last_loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n        else:\n            # inference-time mini-optimization: only forward the output on the very last position\n            logits = self.output(h[:, [-1], :]) # note: using list [-1] to preserve the time dim\n            self.last_loss = None\n\n        return logits\n\n    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n        # start with all of the candidate parameters\n        param_dict = {pn: p for pn, p in self.named_parameters()}\n        # filter out those that do not require grad\n        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n        optim_groups = [\n            {'params': decay_params, 'weight_decay': weight_decay},\n            {'params': nodecay_params, 'weight_decay': 0.0}\n        ]\n        num_decay_params = sum(p.numel() for p in decay_params)\n        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n        # Create optimizer: fused 8-bit AdamW if available and using CUDA, else fallback to CPU AdamW\n        use_fused = (\n            bnb is not None\n            and 'fused' in inspect.signature(bnb.optim.AdamW8bit).parameters\n            and device_type == 'cuda'\n        )\n        if use_fused:\n            optimizer = bnb.optim.AdamW8bit(optim_groups, lr=learning_rate, betas=betas, fused=True)\n            print(\"using fused AdamW8bit\")\n        else:\n            optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas)\n            print(\"using torch.optim.AdamW fallback\")\n\n        return optimizer\n\n    def estimate_mfu(self, fwdbwd_per_iter, dt):\n        \"\"\" estimate model flops utilization (MFU) in units of 4090 bfloat16 peak FLOPS \"\"\"\n        # first estimate the number of flops we do per iteration.\n        # see PaLM paper Appendix B as ref: https://arxiv.org/abs/2204.02311\n        N = sum(p.numel() for p in self.parameters())\n        cfg = self.params\n        L, H, Q, T = cfg.n_layers, cfg.n_heads, cfg.dim//cfg.n_heads, cfg.max_seq_len\n        flops_per_token = 6*N + 12*L*H*Q*T\n        flops_per_fwdbwd = flops_per_token * T\n        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter\n        # express our flops throughput as ratio of A100 bfloat16 peak flops\n        flops_achieved = flops_per_iter * (1.0/dt) # per second\n        flops_promised = 165.2e12 # 4090 GPU bfloat16 peak flops is 312 TFLOPS\n        mfu = flops_achieved / flops_promised\n        return mfu\n\n    @torch.inference_mode()\n    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n        \"\"\"\n        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n        Also note this is a super inefficient version of sampling with no key/value cache.\n        \"\"\"\n        for _ in range(max_new_tokens):\n            # if the sequence context is growing too long we must crop it at block_size\n            idx_cond = idx if idx.size(1) <= self.params.max_seq_len else idx[:, -self.params.max_seq_len:]\n            # forward the model to get the logits for the index in the sequence\n            logits = self(idx_cond)\n            logits = logits[:, -1, :] # crop to just the final time step\n            if temperature == 0.0:\n                # \"sample\" the single most likely index\n                _, idx_next = torch.topk(logits, k=1, dim=-1)\n            else:\n                # pluck the logits at the final step and scale by desired temperature\n                logits = logits / temperature\n                # optionally crop the logits to only the top k options\n                if top_k is not None:\n                    v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n                    logits[logits < v[:, [-1]]] = -float('Inf')\n                # apply softmax to convert logits to (normalized) probabilities\n                probs = F.softmax(logits, dim=-1)\n                idx_next = torch.multinomial(probs, num_samples=1)\n            # append sampled index to the running sequence and continue\n            idx = torch.cat((idx, idx_next), dim=1)\n\n        return idx\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/model.py b/model.py
--- a/model.py	(revision 065ad9979c52136498ed5e444b64c6e63930fa07)
+++ b/model.py	(date 1748022601043)
@@ -11,6 +11,7 @@
 import torch
 import torch.nn.functional as F
 from torch import nn
+from torch.utils.checkpoint import checkpoint
 
 @dataclass
 class ModelArgs:
@@ -257,7 +258,8 @@
         freqs_sin = self.freqs_sin[:seqlen]
 
         for layer in self.layers:
-            h = layer(h, freqs_cos, freqs_sin)
+            # activation checkpointing to cut memory use ~50 %
+            h = checkpoint(layer, h, freqs_cos, freqs_sin)
         h = self.norm(h)
 
         if targets is not None:
Index: train.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>\"\"\"\nThis training script can be run both on a single gpu in debug mode,\nand also in a larger training run with distributed data parallel (ddp).\n\nTo run on a single GPU small debug run, example:\n$ python -m train.py --compile=False --eval_iters=10 --batch_size=8\n\nTo run with DDP on 4 gpus on 1 node, example:\n$ torchrun --standalone --nproc_per_node=4 train.py\n\nTo run with DDP on 4 gpus across 2 nodes, example:\n- Run on the first (master) node with example IP 123.456.123.456:\n$ torchrun --nproc_per_node=8 --nnodes=2 --node_rank=0 --master_addr=123.456.123.456 --master_port=1234 train.py\n- Run on the worker node:\n$ torchrun --nproc_per_node=8 --nnodes=2 --node_rank=1 --master_addr=123.456.123.456 --master_port=1234 train.py\n(If your cluster does not have Infiniband interconnect prepend NCCL_IB_DISABLE=1)\n\"\"\"\n\nimport math\nimport os\nimport time\nfrom contextlib import nullcontext\nfrom datetime import datetime\nfrom functools import partial\n\nimport torch\nfrom model import Transformer, ModelArgs\nfrom torch.distributed import destroy_process_group, init_process_group\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\nfrom finewebedullama2 import Task\nfrom export import model_export\n\n# -----------------------------------------------------------------------------\n# I/O\nout_dir = \"out\"\neval_interval = 2000\nlog_interval = 100\neval_iters = 100\neval_only = False  # if True, script exits right after the first eval\nalways_save_checkpoint = True  # if True, always save a checkpoint after each eval\ninit_from = \"scratch\"  # 'scratch' or 'resume'\n# wandb logging\nwandb_log = True  # disabled by default\nwandb_project = \"llamac\"\nwandb_run_name = \"run\" + datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n# data\nbatch_size = 8  # if gradient_accumulation_steps > 1, this is the micro-batch size\nmax_seq_len = 1024\nvocab_source = \"llama2\" # llama2|custom; use Lllama 2 vocab from Meta, or custom trained\nvocab_size = 32000 # the Llama 2 tokenizer has 32K tokens\n# model\ndim = 1024\nn_layers = 32\nn_heads = 32\nn_kv_heads = 32\nmultiple_of = 32\ndropout = 0.1\n# adamw optimizer\ngradient_accumulation_steps = 64  # used to simulate larger batch sizes\nlearning_rate = 10e-4  # max learning rate\nmax_iters = 200000  # total number of training iterations\nweight_decay = 1e-1\nbeta1 = 0.9\nbeta2 = 0.95\ngrad_clip = 1.0  # clip gradients at this value, or disable if == 0.0\n# learning rate decay settings\ndecay_lr = True  # whether to decay the learning rate\nwarmup_iters = 1000  # how many steps to warm up for\n# system\ndevice = \"cuda\"  # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\ndtype = \"bfloat16\"  # float32|bfloat16|float16\ncompile = True  # use PyTorch 2.0 to compile the model to be faster\n# -----------------------------------------------------------------------------\nconfig_keys = [\n    k\n    for k, v in globals().items()\n    if not k.startswith(\"_\") and isinstance(v, (int, float, bool, str))\n]\nexec(open(\"configurator.py\").read())  # overrides from command line or config file\nconfig = {k: globals()[k] for k in config_keys}  # will be useful for logging\n# -----------------------------------------------------------------------------\n\n# fixing some hyperparams to sensible defaults\nlr_decay_iters = max_iters  # should be ~= max_iters per Chinchilla\nmin_lr = 0.0  # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n\n# validating checks\nassert vocab_source in [\"llama2\", \"custom\"]\nassert vocab_source == \"custom\" or vocab_size == 32000, \"The vocab from Meta has 32K tokens\"\n\n#dataset\n\n\n# various inits, derived attributes, I/O setup\nddp = int(os.environ.get(\"RANK\", -1)) != -1  # is this a ddp run?\nif ddp:\n    init_process_group(backend=\"nccl\")\n    ddp_rank = int(os.environ[\"RANK\"])\n    ddp_local_rank = int(os.environ[\"LOCAL_RANK\"])\n    ddp_world_size = int(os.environ[\"WORLD_SIZE\"])\n    device = f\"cuda:{ddp_local_rank}\"\n    torch.cuda.set_device(device)\n    master_process = ddp_rank == 0  # this process will do logging, checkpointing etc.\n    seed_offset = ddp_rank  # each process gets a different seed\n    # world_size number of processes will be training simultaneously, so we can scale\n    # down the desired gradient accumulation iterations per process proportionally\n    assert gradient_accumulation_steps % ddp_world_size == 0\n    gradient_accumulation_steps //= ddp_world_size\nelse:\n    # if not ddp, we are running on a single gpu, and one process\n    master_process = True\n    seed_offset = 0\n    ddp_world_size = 1\ntokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * max_seq_len\nif master_process:\n    print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n    print(f\"breaks down as: {gradient_accumulation_steps} grad accum steps * {ddp_world_size} processes * {batch_size} batch size * {max_seq_len} max seq len\")\n\nif master_process:\n    os.makedirs(out_dir, exist_ok=True)\ntorch.manual_seed(1337 + seed_offset)\ntorch.backends.cuda.matmul.allow_tf32 = True  # allow tf32 on matmul\ntorch.backends.cudnn.allow_tf32 = True  # allow tf32 on cudnn\ndevice_type = \"cuda\" if \"cuda\" in device else \"cpu\"  # for later use in torch.autocast\n# note: float16 data type will automatically use a GradScaler\nptdtype = {\"float32\": torch.float32, \"bfloat16\": torch.bfloat16, \"float16\": torch.float16}[dtype]\nctx = (\n    nullcontext()\n    if device_type == \"cpu\"\n    else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n)\nprint(ptdtype)\n\ndata=Task(batch_size,device,max_seq_len);\n# task-specific setup\niter_batches = partial(\n    data.iter_batches\n)\n\n# init these up here, can override if init_from='resume' (i.e. from a checkpoint)\niter_num = 0\nbest_val_loss = 1e9\n\n# model init\nmodel_args = dict(\n    dim=dim,\n    n_layers=n_layers,\n    n_heads=n_heads,\n    n_kv_heads=n_kv_heads,\n    vocab_size=vocab_size,\n    multiple_of=multiple_of,\n    max_seq_len=max_seq_len,\n    dropout=dropout,\n)  # start with model_args from command line\nif init_from == \"scratch\":\n    # init a new model from scratch\n    print(\"Initializing a new model from scratch\")\n    gptconf = ModelArgs(**model_args)\n    model = Transformer(gptconf)\nelif init_from == \"resume\":\n    print(f\"Resuming training from {out_dir}\")\n    # resume training from a checkpoint.\n    ckpt_path = os.path.join(out_dir, \"ckpt.pt\")\n    checkpoint = torch.load(ckpt_path, map_location=device,weights_only=False)\n    checkpoint_model_args = checkpoint[\"model_args\"]\n    # force these config attributes to be equal otherwise we can't even resume training\n    # the rest of the attributes (e.g. dropout) can stay as desired from command line\n    for k in [\"dim\", \"n_layers\", \"n_heads\", \"n_kv_heads\", \"vocab_size\", \"multiple_of\", \"max_seq_len\"]:\n        model_args[k] = checkpoint_model_args[k]\n    # create the model\n    gptconf = ModelArgs(**model_args)\n    model = Transformer(gptconf)\n    state_dict = checkpoint[\"model\"]\n    # fix the keys of the state dictionary :(\n    # honestly no idea how checkpoints sometimes get this prefix, have to debug more\n    unwanted_prefix = \"_orig_mod.\"\n    for k, v in list(state_dict.items()):\n        if k.startswith(unwanted_prefix):\n            state_dict[k[len(unwanted_prefix) :]] = state_dict.pop(k)\n    model.load_state_dict(state_dict)\n    iter_num = checkpoint[\"iter_num\"]\n    best_val_loss = checkpoint[\"best_val_loss\"]\nmodel.to(device)\n\n# initialize a GradScaler. If enabled=False scaler is a no-op\nscaler = torch.amp.GradScaler('cuda',enabled=(dtype == \"float16\"))\n\n# optimizer\noptimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\nif init_from == \"resume\" and \"optimizer\" in checkpoint:\n    optimizer.load_state_dict(checkpoint[\"optimizer\"])\ncheckpoint = None  # free up memory\n\n# compile the model\nif compile:\n    print(\"compiling the model... (takes a ~minute)\")\n    unoptimized_model = model\n    model = torch.compile(model)  # requires PyTorch 2.0\n\n# wrap model into DDP container\nif ddp:\n    # Ignore the `freqs_cis` buffer so that DDP does not broadcast it at\n    # construction time since NCCL does not support `ComplexFloat`\n    prefix = \"_orig_mod.\" if compile else \"\"\n    model._ddp_params_and_buffers_to_ignore = {prefix + \"freqs_cis\"}\n    model = DDP(model, device_ids=[ddp_local_rank])\n\n# helps estimate an arbitrarily accurate loss over either split using many batches\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in [\"train\", \"val\"]:\n        batch_iter = iter_batches(split=split)\n        losses = torch.zeros(eval_iters)  # keep on CPU\n        for k in range(eval_iters):\n            X, Y = next(batch_iter)\n            with ctx:\n                logits = model(X, Y)\n                loss = raw_model.last_loss\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out\n\n# learning rate decay scheduler (cosine with warmup)\ndef get_lr(it):\n    # 1) linear warmup for warmup_iters steps\n    if it < warmup_iters:\n        return learning_rate * it / warmup_iters\n    # 2) if it > lr_decay_iters, return min learning rate\n    if it > lr_decay_iters:\n        return min_lr\n    # 3) in between, use cosine decay down to min learning rate\n    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n    assert 0 <= decay_ratio <= 1\n    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))  # coeff ranges 0..1\n    return min_lr + coeff * (learning_rate - min_lr)\n\n# logging\nif wandb_log and master_process:\n    import wandb\n    wandb_logger=wandb.init(project=wandb_project, name=wandb_run_name, config=config)\n\n# training loop\nbatch_generator = iter_batches(split=\"train\",start_index=0)\n# fetch the very first batch\nt0 = time.time()\nlocal_iter_num = 0  # number of iterations in the lifetime of this process\nraw_model = model.module if ddp else model  # unwrap DDP container if needed\nrunning_mfu = -1.0\nX, Y = None, None\ntry:\n    while True:\n        # determine and set the learning rate for this iteration\n        lr = get_lr(iter_num) if decay_lr else learning_rate\n        for param_group in optimizer.param_groups:\n            param_group[\"lr\"] = lr\n    \n        # evaluate the loss on train/val sets and write checkpoints\n        if iter_num % eval_interval == 0 and master_process:\n            losses = estimate_loss()\n            print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n            if wandb_log:\n                try:\n                    wandb_logger.log(\n                        {\n                            \"iter\": iter_num,\n                            \"tokens\": iter_num * tokens_per_iter,\n                            \"loss/train\": losses[\"train\"],\n                            \"loss/val\": losses[\"val\"],\n                            \"lr\": lr,\n                            \"mfu\": running_mfu * 100,  # convert to percentage\n                        }, step = iter_num\n                    )\n                except Exception as e:\n                    print(f\"logging to wandb failed: {e}\")\n            if losses[\"val\"] < best_val_loss or always_save_checkpoint:\n                best_val_loss = losses[\"val\"]\n                if iter_num > 0:\n                    checkpoint = {\n                        \"model\": raw_model.state_dict(),\n                        \"optimizer\": optimizer.state_dict(),\n                        \"model_args\": model_args,\n                        \"iter_num\": iter_num,\n                        \"best_val_loss\": best_val_loss,\n                        \"config\": config,\n                    }\n                    print(f\"saving checkpoint to {out_dir}\")\n                    torch.save(checkpoint, os.path.join(out_dir, \"ckpt.pt\"))\n                    model_export(raw_model, os.path.join(out_dir, \"model.bin\"), version=0)\n            print(torch.cuda.memory_allocated())\n            print(torch.cuda.memory_reserved())\n        if iter_num == 0 and eval_only:\n            break\n        try:\n            if X is None or Y is None or iter_num == 0:\n                X, Y = next(batch_generator)\n        except NameError:\n            X, Y = next(batch_generator)\n\n        try:\n            # forward backward update, with optional gradient accumulation to simulate larger batch size\n            # and using the GradScaler if data type is float16\n            for micro_step in range(gradient_accumulation_steps):\n                if ddp:\n                    # in DDP training we only need to sync gradients at the last micro step.\n                    # the official way to do this is with model.no_sync() context manager, but\n                    # I really dislike that this bloats the code and forces us to repeat code\n                    # looking at the source of that context manager, it just toggles this variable\n                    model.require_backward_grad_sync = micro_step == gradient_accumulation_steps - 1\n                with ctx:\n                    logits = model(X, Y)\n                    loss = raw_model.last_loss\n                    loss = loss / gradient_accumulation_steps\n                # immediately async prefetch next batch while model is doing the forward pass on the GPU\n                X, Y = next(batch_generator)\n                # backward pass, with gradient scaling if training in fp16\n                scaler.scale(loss).backward()\n            # clip the gradient\n            if grad_clip != 0.0:\n                scaler.unscale_(optimizer)\n                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n            # step the optimizer and scaler if training in fp16\n            scaler.step(optimizer)\n            scaler.update()\n            # flush the gradients as soon as we can, no need for this memory anymore\n            optimizer.zero_grad(set_to_none=True)\n        \n            # timing and logging\n            t1 = time.time()\n            dt = t1 - t0\n            t0 = t1\n            if iter_num % log_interval == 0 and master_process:\n                # get loss as float, scale up due to the divide above. note: this is a CPU-GPU sync point\n                lossf = loss.item() * gradient_accumulation_steps\n                if local_iter_num >= 5:  # let the training loop settle a bit\n                    mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n                    running_mfu = mfu if running_mfu == -1.0 else 0.9 * running_mfu + 0.1 * mfu\n                print(\n                    f\"{iter_num} | loss {lossf:.4f} | lr {lr:e} | {dt*1000:.2f}ms | mfu {running_mfu*100:.2f}%\"\n                )\n            iter_num += 1\n            local_iter_num += 1\n        \n            # termination conditions\n        except StopIteration:  # Handle end of dataset\n            print(\"End of data reached.\")\n            break\nexcept KeyboardInterrupt:\n    print(\"Training interrupted manually.\")\n\n\n    \nif ddp:\n    destroy_process_group()\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/train.py b/train.py
--- a/train.py	(revision 065ad9979c52136498ed5e444b64c6e63930fa07)
+++ b/train.py	(date 1748027609339)
@@ -26,7 +26,11 @@
 import torch
 from model import Transformer, ModelArgs
 from torch.distributed import destroy_process_group, init_process_group
-from torch.nn.parallel import DistributedDataParallel as DDP
+# Switched from classic DDP to Fully‑Sharded Data Parallel
+from torch.distributed.fsdp import FullyShardedDataParallel as FSDP
+from torch.distributed.fsdp import MixedPrecision, ShardingStrategy
+
+import bitsandbytes as bnb
 
 from finewebedullama2 import Task
 from export import model_export
@@ -187,25 +191,37 @@
 scaler = torch.amp.GradScaler('cuda',enabled=(dtype == "float16"))
 
 # optimizer
-optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)
+optimizer = bnb.optim.Adam8bit(
+    model.parameters(),
+    lr=learning_rate,
+    betas=(beta1, beta2),
+    weight_decay=weight_decay,
+)
+
 if init_from == "resume" and "optimizer" in checkpoint:
     optimizer.load_state_dict(checkpoint["optimizer"])
 checkpoint = None  # free up memory
 
+
+
+# wrap model into FSDP container (full‑shard for lower VRAM)
+if ddp:
+    mp_policy = MixedPrecision(
+        param_dtype=torch.bfloat16,
+        reduce_dtype=torch.bfloat16,
+        buffer_dtype=torch.bfloat16,
+    )
+    model = FSDP(
+        model,
+        sharding_strategy=ShardingStrategy.FULL_SHARD,
+        mixed_precision=mp_policy,
+        device_id=ddp_local_rank,
+    )
 # compile the model
 if compile:
     print("compiling the model... (takes a ~minute)")
     unoptimized_model = model
     model = torch.compile(model)  # requires PyTorch 2.0
-
-# wrap model into DDP container
-if ddp:
-    # Ignore the `freqs_cis` buffer so that DDP does not broadcast it at
-    # construction time since NCCL does not support `ComplexFloat`
-    prefix = "_orig_mod." if compile else ""
-    model._ddp_params_and_buffers_to_ignore = {prefix + "freqs_cis"}
-    model = DDP(model, device_ids=[ddp_local_rank])
-
 # helps estimate an arbitrarily accurate loss over either split using many batches
 @torch.no_grad()
 def estimate_loss():
@@ -248,7 +264,7 @@
 # fetch the very first batch
 t0 = time.time()
 local_iter_num = 0  # number of iterations in the lifetime of this process
-raw_model = model.module if ddp else model  # unwrap DDP container if needed
+raw_model = model.module if hasattr(model, "module") else model
 running_mfu = -1.0
 X, Y = None, None
 try:
